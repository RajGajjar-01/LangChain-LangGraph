{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "colab-badge",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RajGajjar-01/LangChain-LangGraph/blob/main/Langgraph/16_basic_MCP_client.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colab-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if running in Google Colab\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    !pip install -U langgraph langchain-google-genai langchain-mcp-adapters python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "## **Model Context Protocol (MCP) Client in LangGraph**\n",
    "\n",
    "<div class=\"premium-card\">\n",
    "    This notebook demonstrates how to build a <b>Model Context Protocol (MCP)</b> client using LangGraph. MCP allows agents to interact with external tools and data sources through a standardized protocol. Here, we use a filesystem MCP server to give the agent file management capabilities.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1",
   "metadata": {},
   "source": [
    "### **1. Imports and environment setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddb6c92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import subprocess, textwrap\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from typing import TypedDict\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2",
   "metadata": {},
   "source": [
    "### **2. Sandbox Directory Setup**\n",
    "\n",
    "We create a dedicated directory for the agent to safely perform file operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0dca076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sandbox /home/rajgajjar04/Learnings/Langchain_Langgraph/Langgraph/sandbox_dir\n"
     ]
    }
   ],
   "source": [
    "SANDBOX_DIR = os.path.abspath(\"sandbox_dir\")\n",
    "os.makedirs(SANDBOX_DIR, exist_ok=True)\n",
    "print(\"Sandbox\", SANDBOX_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3",
   "metadata": {},
   "source": [
    "### **3. Connect to MCP Server**\n",
    "\n",
    "We initialize the <code>MultiServerMCPClient</code> to connect to a filesystem server using <code>npx</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8a9c335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- read_file\n",
      "- read_text_file\n",
      "- read_media_file\n",
      "- read_multiple_files\n",
      "- write_file\n",
      "- edit_file\n",
      "- create_directory\n",
      "- list_directory\n",
      "- list_directory_with_sizes\n",
      "- directory_tree\n",
      "- move_file\n",
      "- search_files\n",
      "- get_file_info\n",
      "- list_allowed_directories\n"
     ]
    }
   ],
   "source": [
    "async def test_mcp_list_tools():\n",
    "    client = MultiServerMCPClient(\n",
    "        {\n",
    "            \"filesystem\": {\n",
    "                \"command\": \"npx\",\n",
    "                \"args\": [\n",
    "                    \"-y\",\n",
    "                    \"@modelcontextprotocol/server-filesystem\",\n",
    "                    SANDBOX_DIR\n",
    "                ],\n",
    "                \"transport\": \"stdio\",\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    tools = await client.get_tools()\n",
    "    for t in tools:\n",
    "        print(\"-\", t.name)\n",
    "\n",
    "await test_mcp_list_tools()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4",
   "metadata": {},
   "source": [
    "### **4. Build Graph with MCP Tools**\n",
    "\n",
    "We define the LangGraph structure, integrating tools fetched dynamically from the MCP server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "363e6c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.graph import StateGraph, START\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "async def build_graph_with_mcp_tools():\n",
    "    client = MultiServerMCPClient(\n",
    "        {\n",
    "            \"filesystem\": {\n",
    "                \"command\": \"npx\",\n",
    "                \"args\": [\"-y\", \"@modelcontextprotocol/server-filesystem\", SANDBOX_DIR],\n",
    "                \"transport\": \"stdio\",\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    tools = await client.get_tools()\n",
    "    \n",
    "    for t in tools:\n",
    "        if isinstance(t.args_schema, dict) and \"$schema\" in t.args_schema:\n",
    "            del t.args_schema[\"$schema\"]\n",
    "\n",
    "\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        temperature=0,\n",
    "        google_api_key=os.environ[\"GOOGLE_API_KEY\"],\n",
    "    )\n",
    "\n",
    "    llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "    async def chat(state: dict):\n",
    "        response = await llm_with_tools.ainvoke(state[\"messages\"])\n",
    "        return {\"messages\": state[\"messages\"] + [response]}\n",
    "\n",
    "    tool_node = ToolNode(tools)\n",
    "\n",
    "    g = StateGraph(dict)\n",
    "    g.add_node(\"chatbot\", chat)\n",
    "    g.add_node(\"tools\", tool_node)\n",
    "\n",
    "    g.add_edge(START, \"chatbot\")\n",
    "    g.add_conditional_edges(\"chatbot\", tools_condition)\n",
    "    g.add_edge(\"tools\", \"chatbot\")\n",
    "\n",
    "    return g.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5",
   "metadata": {},
   "source": [
    "### **5. Execute and Verify**\n",
    "\n",
    "Testing the agent by asking it to create and read a file in the sandbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bee59ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "contents are required.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m app = \u001b[38;5;28;01mawait\u001b[39;00m build_graph_with_mcp_tools()\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m app.ainvoke({\n\u001b[32m      4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m      5\u001b[39m         HumanMessage(content=\u001b[33m\"\u001b[39m\u001b[33mCreate sandbox_dir/hello.txt with \u001b[39m\u001b[33m'\u001b[39m\u001b[33mHello I am raj gajjar\u001b[39m\u001b[33m'\u001b[39m\u001b[33m then read it.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m     ]\n\u001b[32m      7\u001b[39m })\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(result[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m][-\u001b[32m1\u001b[39m].content)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Learnings/Langchain_Langgraph/.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:3158\u001b[39m, in \u001b[36mPregel.ainvoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3155\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3156\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3158\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.astream(\n\u001b[32m   3159\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   3160\u001b[39m     config,\n\u001b[32m   3161\u001b[39m     context=context,\n\u001b[32m   3162\u001b[39m     stream_mode=[\u001b[33m\"\u001b[39m\u001b[33mupdates\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   3163\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode == \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3164\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m stream_mode,\n\u001b[32m   3165\u001b[39m     print_mode=print_mode,\n\u001b[32m   3166\u001b[39m     output_keys=output_keys,\n\u001b[32m   3167\u001b[39m     interrupt_before=interrupt_before,\n\u001b[32m   3168\u001b[39m     interrupt_after=interrupt_after,\n\u001b[32m   3169\u001b[39m     durability=durability,\n\u001b[32m   3170\u001b[39m     **kwargs,\n\u001b[32m   3171\u001b[39m ):\n\u001b[32m   3172\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode == \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   3173\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(chunk) == \u001b[32m2\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Learnings/Langchain_Langgraph/.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:2971\u001b[39m, in \u001b[36mPregel.astream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2969\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m loop.amatch_cached_writes():\n\u001b[32m   2970\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2971\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner.atick(\n\u001b[32m   2972\u001b[39m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop.tasks.values() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t.writes],\n\u001b[32m   2973\u001b[39m     timeout=\u001b[38;5;28mself\u001b[39m.step_timeout,\n\u001b[32m   2974\u001b[39m     get_waiter=get_waiter,\n\u001b[32m   2975\u001b[39m     schedule_task=loop.aaccept_push,\n\u001b[32m   2976\u001b[39m ):\n\u001b[32m   2977\u001b[39m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[32m   2978\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m _output(\n\u001b[32m   2979\u001b[39m         stream_mode,\n\u001b[32m   2980\u001b[39m         print_mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2983\u001b[39m         asyncio.QueueEmpty,\n\u001b[32m   2984\u001b[39m     ):\n\u001b[32m   2985\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Learnings/Langchain_Langgraph/.venv/lib/python3.13/site-packages/langgraph/pregel/_runner.py:304\u001b[39m, in \u001b[36mPregelRunner.atick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    302\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    303\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m arun_with_retry(\n\u001b[32m    305\u001b[39m         t,\n\u001b[32m    306\u001b[39m         retry_policy,\n\u001b[32m    307\u001b[39m         stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    308\u001b[39m         configurable={\n\u001b[32m    309\u001b[39m             CONFIG_KEY_CALL: partial(\n\u001b[32m    310\u001b[39m                 _acall,\n\u001b[32m    311\u001b[39m                 weakref.ref(t),\n\u001b[32m    312\u001b[39m                 stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    313\u001b[39m                 retry_policy=retry_policy,\n\u001b[32m    314\u001b[39m                 futures=weakref.ref(futures),\n\u001b[32m    315\u001b[39m                 schedule_task=schedule_task,\n\u001b[32m    316\u001b[39m                 submit=\u001b[38;5;28mself\u001b[39m.submit,\n\u001b[32m    317\u001b[39m                 loop=loop,\n\u001b[32m    318\u001b[39m             ),\n\u001b[32m    319\u001b[39m         },\n\u001b[32m    320\u001b[39m     )\n\u001b[32m    321\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Learnings/Langchain_Langgraph/.venv/lib/python3.13/site-packages/langgraph/pregel/_retry.py:137\u001b[39m, in \u001b[36marun_with_retry\u001b[39m\u001b[34m(task, retry_policy, stream, match_cached_writes, configurable)\u001b[39m\n\u001b[32m    135\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    136\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m task.proc.ainvoke(task.input, config)\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    139\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Learnings/Langchain_Langgraph/.venv/lib/python3.13/site-packages/langgraph/_internal/_runnable.py:705\u001b[39m, in \u001b[36mRunnableSeq.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    703\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    704\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m705\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_task(\n\u001b[32m    706\u001b[39m             step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs), context=context\n\u001b[32m    707\u001b[39m         )\n\u001b[32m    708\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    709\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Learnings/Langchain_Langgraph/.venv/lib/python3.13/site-packages/langgraph/_internal/_runnable.py:473\u001b[39m, in \u001b[36mRunnableCallable.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    471\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m run_manager.on_chain_end(ret)\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m     ret = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.afunc(*args, **kwargs)\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    475\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m ret.ainvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mbuild_graph_with_mcp_tools.<locals>.chat\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mchat\u001b[39m(state: \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m llm_with_tools.ainvoke(state[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: state[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m] + [response]}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Learnings/Langchain_Langgraph/.venv/lib/python3.13/site-packages/langchain_core/runnables/base.py:5570\u001b[39m, in \u001b[36mRunnableBindingBase.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5563\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5564\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m   5565\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5568\u001b[39m     **kwargs: Any | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   5569\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5570\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bound.ainvoke(\n\u001b[32m   5571\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   5572\u001b[39m         \u001b[38;5;28mself\u001b[39m._merge_configs(config),\n\u001b[32m   5573\u001b[39m         **{**\u001b[38;5;28mself\u001b[39m.kwargs, **kwargs},\n\u001b[32m   5574\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Learnings/Langchain_Langgraph/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:421\u001b[39m, in \u001b[36mBaseChatModel.ainvoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    411\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    412\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m    413\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    418\u001b[39m     **kwargs: Any,\n\u001b[32m    419\u001b[39m ) -> AIMessage:\n\u001b[32m    420\u001b[39m     config = ensure_config(config)\n\u001b[32m--> \u001b[39m\u001b[32m421\u001b[39m     llm_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate_prompt(\n\u001b[32m    422\u001b[39m         [\u001b[38;5;28mself\u001b[39m._convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[32m    423\u001b[39m         stop=stop,\n\u001b[32m    424\u001b[39m         callbacks=config.get(\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    425\u001b[39m         tags=config.get(\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    426\u001b[39m         metadata=config.get(\u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    427\u001b[39m         run_name=config.get(\u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    428\u001b[39m         run_id=config.pop(\u001b[33m\"\u001b[39m\u001b[33mrun_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    429\u001b[39m         **kwargs,\n\u001b[32m    430\u001b[39m     )\n\u001b[32m    431\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    432\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m, cast(\u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m, llm_result.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]).message\n\u001b[32m    433\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Learnings/Langchain_Langgraph/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:1128\u001b[39m, in \u001b[36mBaseChatModel.agenerate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1119\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1120\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34magenerate_prompt\u001b[39m(\n\u001b[32m   1121\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1125\u001b[39m     **kwargs: Any,\n\u001b[32m   1126\u001b[39m ) -> LLMResult:\n\u001b[32m   1127\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate(\n\u001b[32m   1129\u001b[39m         prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n\u001b[32m   1130\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Learnings/Langchain_Langgraph/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:1086\u001b[39m, in \u001b[36mBaseChatModel.agenerate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m   1073\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[32m   1074\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m   1075\u001b[39m             *[\n\u001b[32m   1076\u001b[39m                 run_manager.on_llm_end(\n\u001b[32m   (...)\u001b[39m\u001b[32m   1084\u001b[39m             ]\n\u001b[32m   1085\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1086\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions[\u001b[32m0\u001b[39m]\n\u001b[32m   1087\u001b[39m flattened_outputs = [\n\u001b[32m   1088\u001b[39m     LLMResult(generations=[res.generations], llm_output=res.llm_output)  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m   1089\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[32m   1090\u001b[39m ]\n\u001b[32m   1091\u001b[39m llm_output = \u001b[38;5;28mself\u001b[39m._combine_llm_outputs([res.llm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Learnings/Langchain_Langgraph/.venv/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:1339\u001b[39m, in \u001b[36mBaseChatModel._agenerate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1337\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1338\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._agenerate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1339\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agenerate(\n\u001b[32m   1340\u001b[39m         messages, stop=stop, run_manager=run_manager, **kwargs\n\u001b[32m   1341\u001b[39m     )\n\u001b[32m   1342\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1343\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agenerate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Learnings/Langchain_Langgraph/.venv/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:3081\u001b[39m, in \u001b[36mChatGoogleGenerativeAI._agenerate\u001b[39m\u001b[34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[39m\n\u001b[32m   3067\u001b[39m request = \u001b[38;5;28mself\u001b[39m._prepare_request(\n\u001b[32m   3068\u001b[39m     messages,\n\u001b[32m   3069\u001b[39m     stop=stop,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3077\u001b[39m     **kwargs,\n\u001b[32m   3078\u001b[39m )\n\u001b[32m   3079\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   3080\u001b[39m     response: GenerateContentResponse = (\n\u001b[32m-> \u001b[39m\u001b[32m3081\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.client.aio.models.generate_content(\n\u001b[32m   3082\u001b[39m             **request,\n\u001b[32m   3083\u001b[39m         )\n\u001b[32m   3084\u001b[39m     )\n\u001b[32m   3085\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ClientError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   3086\u001b[39m     _handle_client_error(e, request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Learnings/Langchain_Langgraph/.venv/lib/python3.13/site-packages/google/genai/models.py:7006\u001b[39m, in \u001b[36mAsyncModels.generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   6998\u001b[39m     indices_str = \u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m, incompatible_tools_indexes))\n\u001b[32m   6999\u001b[39m     logger.warning(\n\u001b[32m   7000\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mTools at indices [\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m] are not compatible with automatic function \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   7001\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mcalling (AFC). AFC is disabled. If AFC is intended, please \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   7004\u001b[39m         indices_str,\n\u001b[32m   7005\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m7006\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._generate_content(\n\u001b[32m   7007\u001b[39m       model=model, contents=contents, config=parsed_config\n\u001b[32m   7008\u001b[39m   )\n\u001b[32m   7009\u001b[39m remaining_remote_calls_afc = _extra_utils.get_max_remote_calls_afc(\n\u001b[32m   7010\u001b[39m     parsed_config\n\u001b[32m   7011\u001b[39m )\n\u001b[32m   7012\u001b[39m logger.info(\n\u001b[32m   7013\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mAFC is enabled with max remote calls: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mremaining_remote_calls_afc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   7014\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Learnings/Langchain_Langgraph/.venv/lib/python3.13/site-packages/google/genai/models.py:5800\u001b[39m, in \u001b[36mAsyncModels._generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   5798\u001b[39m     path = \u001b[33m'\u001b[39m\u001b[38;5;132;01m{model}\u001b[39;00m\u001b[33m:generateContent\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   5799\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m5800\u001b[39m   request_dict = \u001b[43m_GenerateContentParameters_to_mldev\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5801\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_api_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameter_model\u001b[49m\n\u001b[32m   5802\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5803\u001b[39m   request_url_dict = request_dict.get(\u001b[33m'\u001b[39m\u001b[33m_url\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m   5804\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m request_url_dict:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Learnings/Langchain_Langgraph/.venv/lib/python3.13/site-packages/google/genai/models.py:1304\u001b[39m, in \u001b[36m_GenerateContentParameters_to_mldev\u001b[39m\u001b[34m(api_client, from_object, parent_object)\u001b[39m\n\u001b[32m   1292\u001b[39m   setv(\n\u001b[32m   1293\u001b[39m       to_object,\n\u001b[32m   1294\u001b[39m       [\u001b[33m'\u001b[39m\u001b[33m_url\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m   1295\u001b[39m       t.t_model(api_client, getv(from_object, [\u001b[33m'\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m'\u001b[39m])),\n\u001b[32m   1296\u001b[39m   )\n\u001b[32m   1298\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m getv(from_object, [\u001b[33m'\u001b[39m\u001b[33mcontents\u001b[39m\u001b[33m'\u001b[39m]) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1299\u001b[39m   setv(\n\u001b[32m   1300\u001b[39m       to_object,\n\u001b[32m   1301\u001b[39m       [\u001b[33m'\u001b[39m\u001b[33mcontents\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m   1302\u001b[39m       [\n\u001b[32m   1303\u001b[39m           _Content_to_mldev(item, to_object)\n\u001b[32m-> \u001b[39m\u001b[32m1304\u001b[39m           \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mt_contents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgetv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfrom_object\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcontents\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1305\u001b[39m       ],\n\u001b[32m   1306\u001b[39m   )\n\u001b[32m   1308\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m getv(from_object, [\u001b[33m'\u001b[39m\u001b[33mconfig\u001b[39m\u001b[33m'\u001b[39m]) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1309\u001b[39m   setv(\n\u001b[32m   1310\u001b[39m       to_object,\n\u001b[32m   1311\u001b[39m       [\u001b[33m'\u001b[39m\u001b[33mgenerationConfig\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1314\u001b[39m       ),\n\u001b[32m   1315\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Learnings/Langchain_Langgraph/.venv/lib/python3.13/site-packages/google/genai/_transformers.py:518\u001b[39m, in \u001b[36mt_contents\u001b[39m\u001b[34m(contents)\u001b[39m\n\u001b[32m    512\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mt_contents\u001b[39m(\n\u001b[32m    513\u001b[39m     contents: Optional[\n\u001b[32m    514\u001b[39m         Union[types.ContentListUnion, types.ContentListUnionDict, types.Content]\n\u001b[32m    515\u001b[39m     ],\n\u001b[32m    516\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m[types.Content]:\n\u001b[32m    517\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m contents \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(contents, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m contents):\n\u001b[32m--> \u001b[39m\u001b[32m518\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mcontents are required.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    519\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(contents, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m    520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [t_content(contents)]\n",
      "\u001b[31mValueError\u001b[39m: contents are required.",
      "During task with name 'chatbot' and id '2ccb9456-8724-671c-c81e-3d80743f5cba'"
     ]
    }
   ],
   "source": [
    "app = await build_graph_with_mcp_tools()\n",
    "\n",
    "result = await app.ainvoke({\n",
    "    \"messages\": [\n",
    "        HumanMessage(content=\"Create sandbox_dir/hello.txt with 'Hello I am raj gajjar' then read it.\")\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(result[\"messages\"][-1].content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-langgraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}