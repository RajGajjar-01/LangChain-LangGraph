{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "colab-badge",
            "metadata": {},
            "source": [
                "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RajGajjar-01/LangChain-LangGraph/blob/main/Langgraph/14_Postgres_Checkpoint.ipynb)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "colab-setup",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies if running in Google Colab\n",
                "import sys\n",
                "if 'google.colab' in sys.modules:\n",
                "    !pip install -U langgraph langchain-google-genai langchain-huggingface python-dotenv psycopg[binary] langchain-tavily"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "84292da0",
            "metadata": {},
            "source": [
                "## **LangGraph: Checkpointing with PostgreSQL**\n",
                "\n",
                "This notebook demonstrates how to implement persistent memory in LangGraph using a PostgreSQL checkpointer. This allows the graph to \"remember\" conversation history across different threads and sessions, even if the process restarts.\n",
                "\n",
                "### **1. Imports and environment setup**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0efdf244",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import requests\n",
                "from langchain_core.tools import tool\n",
                "from dotenv import load_dotenv"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6321276f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Database configuration\n",
                "# Note: Ensure you have a running PostgreSQL instance\n",
                "DB_URI = \"postgresql://postgres:postgres@localhost:5432/postgres\"\n",
                "load_dotenv()\n",
                "\n",
                "OPENWEATHER_API_KEY = os.getenv('OPENWEATHER_API_KEY')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "llm-setup-intro",
            "metadata": {},
            "source": [
                "### **2. Initialize LLM**\n",
                "\n",
                "We use Google's Gemini Flash model for this implementation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "48f18a38",
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_google_genai import ChatGoogleGenerativeAI\n",
                "\n",
                "llm = ChatGoogleGenerativeAI(\n",
                "    model = 'gemini-2.5-flash'\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "tool-definition-intro",
            "metadata": {},
            "source": [
                "### **3. Define Tools**\n",
                "\n",
                "We define a custom weather tool and a search tool."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a7232ae9",
            "metadata": {},
            "outputs": [],
            "source": [
                "@tool\n",
                "def get_weather(location: str) -> str:\n",
                "    \"\"\"Get the weather for a specific location.\"\"\"\n",
                "    if not OPENWEATHER_API_KEY:\n",
                "        return 'ERROR: Openweather API key not set. Please set OPENWEATHER_API_KEY environment variable'\n",
                "    \n",
                "    base_url = \"http://api.openweathermap.org/data/2.5/weather\"\n",
                "    params = {\n",
                "        \"q\" : location,\n",
                "        \"appid\": OPENWEATHER_API_KEY,\n",
                "        \"units\": \"metric\"\n",
                "    }\n",
                "    try:\n",
                "        response = requests.get(base_url, params=params, timeout=10)\n",
                "        response.raise_for_status()\n",
                "        data = response.json()\n",
                "        city = data['name']\n",
                "        country = data['sys']['country']\n",
                "        temp = data['main']['temp']\n",
                "        feels_like = data['main']['feels_like']\n",
                "        humidity = data['main']['humidity']\n",
                "        description = data['weather'][0]['description']\n",
                "        wind_speed = data['wind']['speed']\n",
                "        \n",
                "        weather_info = f\"\"\"\n",
                "            Weather in {city}, {country}: \n",
                "            Temperature: {temp} degree celsius\n",
                "            Condition: {description.title()}\n",
                "            Feels Like: {feels_like}\n",
                "            Humidity: {humidity} %\n",
                "            windSpeed : {wind_speed} m/s \n",
                "        \"\"\".strip()\n",
                "        return weather_info\n",
                "    except Exception as e:\n",
                "        return f'Error fetching weather: {str(e)}'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5011cd32",
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_tavily import TavilySearch\n",
                "\n",
                "search_tool = TavilySearch(\n",
                "    max_results=5,\n",
                "    topic='general'\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "checkpointer-setup-intro",
            "metadata": {},
            "source": [
                "### **4. Persistent Memory (PostgreSQL Checkpointer)**\n",
                "\n",
                "Setting up the connection to PostgreSQL to store the graph state."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d97bfbdc",
            "metadata": {},
            "outputs": [],
            "source": [
                "from psycopg import connect\n",
                "from psycopg.rows import dict_row\n",
                "from langgraph.checkpoint.postgres import PostgresSaver\n",
                "\n",
                "# Establish connection\n",
                "conn = connect(\n",
                "    DB_URI,\n",
                "    autocommit=True,\n",
                "    row_factory=dict_row\n",
                ")\n",
                "\n",
                "# Initialize checkpointer\n",
                "checkpointer = PostgresSaver(conn)\n",
                "checkpointer.setup()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "agent-setup-intro",
            "metadata": {},
            "source": [
                "### **5. Build the Agent**\n",
                "\n",
                "Defining the system prompt and creating the agent with persistent memory."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ae809613",
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain.messages import SystemMessage\n",
                "\n",
                "system_prompt=SystemMessage(\n",
                "    content=\"\"\"You are a friendly conversational AI assistant. \n",
                "    Your mission is to answer user questions using tools if necessary.\n",
                "    Ground your answers in credible data and provide markdown formatting.\n",
                "    \"\"\"\n",
                ")\n",
                "\n",
                "from langchain.agents import create_agent\n",
                "\n",
                "tools = [get_weather, search_tool]\n",
                "\n",
                "agent = create_agent(\n",
                "    model=llm,\n",
                "    tools=tools,\n",
                "    checkpointer=checkpointer,\n",
                "    system_prompt=system_prompt\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "chat-function-intro",
            "metadata": {},
            "source": [
                "### **6. Interaction Loop**\n",
                "\n",
                "Function to handle conversations while maintaining state via `thread_id`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a7b8ef76",
            "metadata": {},
            "outputs": [],
            "source": [
                "def chat(message: str, thread_id: str = 'conversation-1'):\n",
                "    config = {\n",
                "        \"configurable\": {\n",
                "            \"thread_id\": thread_id\n",
                "        }\n",
                "    }\n",
                "\n",
                "    print(f\"\\nUser: {message}\")\n",
                "\n",
                "    result = agent.invoke(\n",
                "        {\"messages\": [{\"role\": \"user\", \"content\": message}]},\n",
                "        config=config\n",
                "    )\n",
                "\n",
                "    last_message = result[\"messages\"][-1]\n",
                "    print(f\"Assistant: {last_message.content}\")\n",
                "        \n",
                "    return result"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "execution-intro",
            "metadata": {},
            "source": [
                "### **7. Run Example**\n",
                "\n",
                "Testing persistence by introduce ourselves."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4683c0d3",
            "metadata": {},
            "outputs": [],
            "source": [
                "chat('Hi! my name is Raj Gajjar')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3ba322f1",
            "metadata": {},
            "outputs": [],
            "source": [
                "chat('What is my name ?')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "langchain-langgraph",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}